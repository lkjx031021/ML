## 深度学习模型的压缩

- 设计一个合理的深度学习模型
- 降低计算精度（float32 -> float16）
  - 深度学习模型对于计算精度要求不高
  - 对于指数位有一定要求：bfloat16
  - 模型压缩后可以完全放到Cache中 极大的提升计算速度
- 建模过程中的历史
  - 感受野对于模型的精度比模型复杂度重要（只要保证感受野是足够的精度就能保证，）
  - AlexNet：第一层卷积核大小11*11
  - VGGNet：卷积核大小 3×3+3×3 多个3×3的卷积核组成，见笑了很多参数，提升了计算速度  可训练参数数量 1000W所有
  - GoogleNet：多个分支结构   参数数量减少了一个数量级 可训练参数几百万左右
  - 空洞卷积（新的卷积结构）：比较简单的一种优化方式，感受野不变，但是可训练参数大大减少
  - 深度可分离卷积（设计理念来自于MobileNet）：为每一个特征图设计一个卷积核，那么原来卷积层参数为3×3×128×128变为3×3×128，逐点卷积  1×1×128
- 模型蒸馏和剪枝
  - 统计权值 大部分权值接近0 （较大的权值对于模型的影响较大，把接近0的权值剪掉，这样会使模型可训练参数的数量大大减少）
  - 模型蒸馏（把深得网络当作教师，教浅层网络，让浅层网络的输出值接近深层网络）
  - ![image-20200224222937727](C:\Users\lkjx0\AppData\Roaming\Typora\typora-user-images\image-20200224222937727.png)
  - 





## 对抗生成网络

1. 思考：如何通过标签生成手写数字
   - 输入：标签（onehot）
   - 输出：手写数字
   - 设计网络：反卷积 
   - loss函数：网络生成的图片和手写的图片可以对应上 （MSE）
   - 输入固定：但是输出是半随机的
   - 但是神经网络中不存在随机性
2. 改进1：在输入中加入随机性
   - 使用MSE作为loss 意味着每个像素都要对应上
   - 约束较强，对于低频特征约束
3. 改进2：引入判别网络，对结果进行判定
   - 引入判别网络，判别生成的数字是否是所需结果
   - 对于高频特征约束较强
   - 





## 图像问题的编码解码结构

- 编码器
- 解码器
- 可以完成图像去噪
- 



## 模型的正则化和优化结构

-  weight decay l1、l2正则化

  - 在正常损失函数的基础上加入正则化

    ```python
    loss = tf.reduce_mean((logits-label)**2)# 获取所有可训练参数列表
    all_var = tf.trainable_variables()# 获取所有可训练参数列表
    reg_loss = 0
    for var in all_var:
    	reg_loss = tf.reduct_mean(var**2)
        # reg_loss = tf.reduct_mean(tf.abs(var))
    loss = loss + 1e-6 *reg_loss
    ```

    

- DropOut
  
  -  缓解过拟合
  -  加快训练
  -  大多数应用于全连接层中
  -  推断和训练是不同的
    - x * r    x 和r都是向量 r 是0、1向量 dropout比例为0.8的话 那么向量r中80%是8
    - 在训练过程中所有的向量都进行输出，但是需要x * 0.8
    - 相当于每次训练只训练部分权值，权值不发生变化
    - 训练和推断过程中的training参数需要设置
  
- BatchNormal（优化结构）
  - 加快训练速度
  - 可以在一定程度上避免过拟合
  - 如何做的
  - 训练和推断是不同的
    - 训练过程：均值和标准差均来自于本批样本
      - $x_y:第t次迭代的样本 \mu_t 第t次迭代样本的均值，\sigma_t$
    - 推断过程：在线学习的方式
      - $\mu=\beta \mu +(1-\beta)\mu_t$    $\mu为总体均值\beta通常=0.99，标准差也是如此计算$
    - 推断过程中使用训练过程中保存的均值和方差
  - 知道API中每个参数的含义