## 深度学习模型的压缩

- 设计一个合理的深度学习模型
- 降低计算精度（float32 -> float16）
  - 深度学习模型对于计算精度要求不高
  - 对于指数位有一定要求：bfloat16
  - 模型压缩后可以完全放到Cache中 极大的提升计算速度
- 建模过程中的历史
  - 感受野对于模型的精度比模型复杂度重要（只要保证感受野是足够的精度就能保证，）
  - AlexNet：第一层卷积核大小11*11
  - VGGNet：卷积核大小 3×3+3×3 多个3×3的卷积核组成，见笑了很多参数，提升了计算速度  可训练参数数量 1000W所有
  - GoogleNet：多个分支结构   参数数量减少了一个数量级 可训练参数几百万左右
  - 空洞卷积（新的卷积结构）：比较简单的一种优化方式，感受野不变，但是可训练参数大大减少
  - 深度可分离卷积（设计理念来自于MobileNet）：为每一个特征图设计一个卷积核，那么原来卷积层参数为3×3×128×128变为3×3×128，逐点卷积  1×1×128
- 模型蒸馏和剪枝
  - 统计权值 大部分权值接近0 （较大的权值对于模型的影响较大，把接近0的权值剪掉，这样会使模型可训练参数的数量大大减少）
  - 模型蒸馏（把深得网络当作教师，教浅层网络，让浅层网络的输出值接近深层网络）
  - ![image-20200224222937727](C:\Users\lkjx0\AppData\Roaming\Typora\typora-user-images\image-20200224222937727.png)
  - 





## 对抗生成网络

1. 思考：如何通过标签生成手写数字
   - 输入：标签（onehot）
   - 输出：手写数字
   - 设计网络：反卷积 
   - loss函数：网络生成的图片和手写的图片可以对应上 （MSE）
   - 输入固定：但是输出是半随机的
   - 但是神经网络中不存在随机性
2. 改进1：在输入中加入随机性
   - 使用MSE作为loss 意味着每个像素都要对应上
   - 约束较强，对于低频特征约束
3. 改进2：引入判别网络，对结果进行判定
   - 引入判别网络，判别生成的数字是否是所需结果
   - 对于高频特征约束较强





## 图像问题的编码解码结构

- 编码器
- 解码器
- 可以完成图像去噪
- 