## 基本记号

- 设训练数据集为$D$， $|D|$表示样本个数
- 设有$K$个类$C_k， k = 1,2,3,4\ldots ，|C_k|$为属于类$C_k$的样本个数，有：$\sum_k|C_k|=|D|
- 设特征$A$有$n$个不同的取值$\{a_1,a_2,a_3 \ldots a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2 \ldots D_n，|D_i|$为$D_i$的样本个数，有$\sum_i|D_i|=|D|$
- 记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，$|D_{ik}|$为$D_{ik}$的样本个数。



## 熵

用来衡量一个事件的不确定程度，信息量

$H(X,Y)-H(X)=H(Y|X)=-\sum_{x,y}p(x,y) \log p(y|x)$



## ID3

使用信息增益/呼吸逆袭$g(D,A)$进行特征选择

- 取值多的属性，更容易使数据更纯，其信息增益更大。
- 训练得到的是一颗庞大且深度浅的树：不合理

## C.5

利用的是信息增益率，信息增益再除以特征本身的熵

$g_r(D,A)=g(D,A) / H(A)$

## CART

使用基尼系数

## 基尼系数(机器学习中的基尼系数)

$$
\begin{align}
Gini(p)&=\sum_{k=1}^Kp_k(1-p_k) \\
&=1-\sum_{k=1}^Kp_k^2 \\
&=1-\sum_{k=1}^K\left(\frac{|C_k|}{|D|} \right)^2
\end{align}
$$

- 将$f(x)=- \ln x在x=1处一阶泰勒展开，忽略高阶无穷小，得到f(x)\approx 1-x$

$$
\begin{align}
H(X)=-\sum_{k=1}^Kp_k\ln p_k \approx\sum_{k=1}^Kp_k(1-p_k)
\end{align}
$$

一个属性的信息增益（率）/gini指数越大，表明属性对样本的熵减少的能力更强，这个属性使得数据由不确定变成确定性的能力越强。