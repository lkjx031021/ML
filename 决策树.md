## 基本记号

- 设训练数据集为$D$， $|D|$表示样本个数
- 设有$K$个类$C_k， k = 1,2,3,4\ldots ，|C_k|$为属于类$C_k$的样本个数，有：$\sum_k|C_k|=|D|
- 设特征$A$有$n$个不同的取值$\{a_1,a_2,a_3 \ldots a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2 \ldots D_n，|D_i|$为$D_i$的样本个数，有$\sum_i|D_i|=|D|$
- 记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，$|D_{ik}|$为$D_{ik}$的样本个数。



## 熵

用来衡量一个事件的不确定程度，信息量

$H(X,Y)-H(X)=H(Y|X)=-\sum_{x,y}p(x,y) \log p(y|x)$



## 构建过程

1. 计算每个特征的信息增益
2. 选择信息增益最大的那个特征当作当前节点进行分类
3. 根据该特征的属性值，划分成n个子节点
4. 以此类推

## ID3

使用信息增益/互信息$g(D,A)$进行特征选择

- 取值多的属性，更容易使数据更纯，其信息增益更大。
- 训练得到的是一颗庞大且深度浅的树：不合理

1. 计算数据集D的经验熵$H(D)$
   $$
   H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log _2 \frac{|C_k|}{|D|}
   $$

2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$ $n$是指该特征下特征的种类，及划分的节点数目，子树的数量
   $$
   H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log _2\frac{|D_{ik}|}{|D_i|}
   $$
   
3. 计算信息增益
   $$
   g(D,A) = H(D)-H(D|A)
   $$



例：假设目前有11个样本，其中7个正样本，4个负样本，该决策树的根节点，及整个数据集的经验熵为：$H(D)=-(\frac{7}{11}\log \frac{7}{11} + \frac{4}{11}\log \frac{4}{11})$，此时根据颜色这个特征进行划分样本，假如共有黑白两种颜色，白：6，黑：5，那么子树有两个，黑子树和白子树。其中白子树有5个正样本，一个负样本，黑子树有1个正样本，4个负样本，那么根据颜色划分之后的熵为：$H(D|颜色)=-(\frac{6}{11}(-(\frac{5}{6}\log \frac{5}{6}+\frac{1}{6}\log \frac{1}{6}))+\frac{5}{11}(-(\frac{1}{5}\log \frac{1}{5}+\frac{4}{5}\log \frac{4}{5})))$，那么信息增益为$g(D,颜色)=H(D)-H(D|颜色)$，以此类推，计算每个特征的信息增益，选择信息增益大的特征作为根节点，之后每个子树在进行划分，也是按照这个流程。



## C4.5

因为在ID3中一般会优先选择特征属性较多的列（原因：假如选择自增id作为一个特征，那么在进行子节点划分的时候，每个id对应的样本标签都只有一个，此时熵为0，信息增益最大），所以C4.5为了规避这一缺陷，使用的是信息增益率，信息增益再除以特征本身的熵

$g_r(D,A)=g(D,A) / H(A)$

$H(A)$就是上例中的$H(颜色)=-(\frac{6}{11}\log \frac{6}{11}+\frac{5}{11}\log \frac{5}{11})$

## CART

使用基尼系数

## 基尼系数(机器学习中的基尼系数)

$$
\begin{align}
Gini(p)&=\sum_{k=1}^Kp_k(1-p_k) \\
&=1-\sum_{k=1}^Kp_k^2 \\
&=1-\sum_{k=1}^K\left(\frac{|C_k|}{|D|} \right)^2
\end{align}
$$

- 将$f(x)=- \ln x在x=1处一阶泰勒展开，忽略高阶无穷小，得到f(x)\approx 1-x$

$$
\begin{align}
H(X)=-\sum_{k=1}^Kp_k\ln p_k \approx\sum_{k=1}^Kp_k(1-p_k)
\end{align}
$$

基尼系数可以认为是熵的一阶近似

一个属性的信息增益（率）/gini指数越大，表明属性对样本的熵减少的能力更强，这个属性使得数据由不确定变成确定性的能力越强。

## 决策树回归如何确定分界阈值

1. 找到特征的最小值和最大值，等分N份，逐个尝试，选择MSE最小的那个阈值
2. N个特征共有N-1个区间，每个区间逐个尝试
3. 在区间[min, max]中随机选n个值进行阈值划分，选择MSE最小的那个阈值



## 决策树过拟合解决

1. ### 剪枝

   1. 预剪枝
      1. 所有决策树的构建方法都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，比如信息增益小于这个阈值，即使还可以继续降低熵，也停止分支的创建
      2. 控制树的深度（最直观，最有用），max_depth参数（越深模型拟合能力越强，越容易过拟合）
      3. min_samples_split，叶子节点样本数，最少多少样本才往下分
      4. min_samples_leaf，某叶子节点分支后，得到的每个子节点样本数目都大于10则完成分支，否则不进行分支
      5. class_weight， 样本权重
   2. 后剪枝
      1. 对拥有同样父节点的一组节点进行检查，判断如果将其合并，信息增益是否小于某一阈值，如果满足则可以合并节点
         1. 对于树中每一个非叶子节点的子树，尝试把他替换成一个叶子节点
         2. 该叶子节点的类别用子树覆盖训练样本中存在最多的那个类别来代替，产生简化决策树
         3. 然后比较这两个决策树在测试集中的表现
         4. 简化决策树在测试集中的错误比较少，那么该子树就可以替换成叶子节点
         5. 以bootom-up的方式便利所有的子树，当没有任何子树可以替换提升时，算法终止

2. ### 随机森林

   - 采样 
     
     - 假设共有$N$个样本，每次有放回地随机选取一个样本，共选取$\alpha N$个样本进行一棵树的训练。每个样本被选中的概率是$\frac{1}{N}$ 未被选中的概率是$1-\frac{1}{N}$，连续$N$次未被选中的概率是$(1-\frac{1}{N})^N$，被选中的概率是$1-(1-\frac{1}{N})^N=1-\frac{1}{e}\approx 0.632$
     
   - 每棵树采用随机采样的方式划分数据集，保证每棵树的训练数据都不完全一样，多棵树之间并无关联，组成森林
   
   - 预测时，每颗决策树各自判断，产生自己的预测结果，被预测出次数最多的那一类别当作最终的预测类别（投票分类的方式）
   
   - OOB数据
   
     - Bootstrap每次约有$36.79\%$的样本不会出现在Botstrap所采集的样本集合中，将未参与模型训练的数据成为袋外数据OOB（Out Of Bag）。它可以用于取代测试集用于误差估计。
       - Breiman以经验性实例的形式证明袋外数据误差估计与同训练集一样大小的测试集精度相同；
       - 得到的模型参数是无偏估计
   
   - 构造过程
   
     - 从样本集中用Bootstrap采样选出n个样本；
     - 从所有属性中随机选择k个属性，选择最佳分割属性做为节点建立决策树
     - 重复以上步骤m次，即建立了m颗决策树
     - 这m个决策树形成随机森林，通过投票表决结果，决定数据属于哪一类
   
   - 投票机制
   
     - 简单投票机制
   
       - 一票否决
       - 少数服从多数
         - 有效多数（加权）
       - 阈值表决
   
     - 贝叶斯投票机制
   
       例（一种方案）：
       $$
       WR = \frac{v}{v+m}R+\frac{m}{v+m}C
       $$
       $WR$：加权得分（weighted rating）
   
       $R$：该电影的用户投票的平均得分（Rating）
   
       $C$：所有电影的平均得分
   
       $v$：该电影的投票人数（vates）
   
       $m$：排名前250名的电影的最低投票数
   
       - 根据总投票人数，250可能有所调整
       - 按照v=0和m=0分别分析