## 基本记号

- 设训练数据集为$D$， $|D|$表示样本个数
- 设有$K$个类$C_k， k = 1,2,3,4\ldots ，|C_k|$为属于类$C_k$的样本个数，有：$\sum_k|C_k|=|D|
- 设特征$A$有$n$个不同的取值$\{a_1,a_2,a_3 \ldots a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2 \ldots D_n，|D_i|$为$D_i$的样本个数，有$\sum_i|D_i|=|D|$
- 记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，$|D_{ik}|$为$D_{ik}$的样本个数。



## 熵

用来衡量一个事件的不确定程度，信息量

$H(X,Y)-H(X)=H(Y|X)=-\sum_{x,y}p(x,y) \log p(y|x)$



## ID3

使用信息增益/互信息$g(D,A)$进行特征选择

- 取值多的属性，更容易使数据更纯，其信息增益更大。
- 训练得到的是一颗庞大且深度浅的树：不合理

1. 计算数据集D的经验熵$H(D)$
   $$
   H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log _2 \frac{|C_k|}{|D|}
   $$

2. 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$ $n$是指该特征下特征的种类，及划分的节点数目，子树的数量
   $$
   H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log _2\frac{|D_{ik}|}{|D_i|}
   $$
   

3. 计算信息增益
   $$
   g(D,A) = H(D)-H(D|A)
   $$



例：假设目前有11个样本，其中7个正样本，4个负样本，该决策树的根节点，及整个数据集的经验熵为：$H(D)=-(\frac{7}{11}\log \frac{7}{11} + \frac{4}{11}\log \frac{4}{11})$，此时根据颜色这个特征进行划分样本，假如共有黑白两种颜色，白：6，黑：5，那么子树有两个，黑子树和白子树。其中白子树有5个正样本，一个负样本，黑子树有1个正样本，4个负样本，那么根据颜色划分之后的熵为：$H(D|颜色)=-(\frac{6}{11}(-(\frac{5}{6}\log \frac{5}{6}+\frac{1}{6}\log \frac{1}{6}))+\frac{5}{11}(-(\frac{1}{5}\log \frac{1}{5}+\frac{4}{5}\log \frac{4}{5})))$，那么信息增益为$g(D,颜色)=H(D)-H(D|颜色)$，以此类推，计算每个特征的信息增益，选择信息增益大的特征作为根节点，之后每个子树在进行划分，也是按照这个流程。



## C4.5

因为在ID3中一般会优先选择特征属性较多的列（原因：假如选择自增id作为一个特征，那么在进行子节点划分的时候，每个id对应的样本标签都只有一个，此时熵为0，信息增益最大），所以C4.5为了规避这一缺陷，使用的是信息增益率，信息增益再除以特征本身的熵

$g_r(D,A)=g(D,A) / H(A)$

$H(A)$就是上例中的$H(颜色)=-(\frac{6}{11}\log \frac{6}{11}+\frac{5}{11}\log \frac{5}{11})$

## CART

使用基尼系数

## 基尼系数(机器学习中的基尼系数)

$$
\begin{align}
Gini(p)&=\sum_{k=1}^Kp_k(1-p_k) \\
&=1-\sum_{k=1}^Kp_k^2 \\
&=1-\sum_{k=1}^K\left(\frac{|C_k|}{|D|} \right)^2
\end{align}
$$

- 将$f(x)=- \ln x在x=1处一阶泰勒展开，忽略高阶无穷小，得到f(x)\approx 1-x$

$$
\begin{align}
H(X)=-\sum_{k=1}^Kp_k\ln p_k \approx\sum_{k=1}^Kp_k(1-p_k)
\end{align}
$$

一个属性的信息增益（率）/gini指数越大，表明属性对样本的熵减少的能力更强，这个属性使得数据由不确定变成确定性的能力越强。