万能近似定理

## 常见数据类型

- 表格类型，[样本数量, 特征数量]
  - 特点：属性之间没有顺序关系
  - 处理方法：传统机器学习方法
- 文本类型数据，
  - 统计词频，无顺序
  - 有顺序的文本： 隐式马尔科夫链，RNN
  - 延申：音频
- 图像类型数据
  - 特征工程：频谱变换
  - 机器学习方法：CNN
  - 延伸：视频，雷达



## 梯度消失问题：

- 训练样本数量与可训练参数数量要在一个量级（否则容易过拟合）

- 梯度消失的根本原因：

  ​		数据输出的均值不是0

- Relu可以在一定程度上解决梯度消失的问题

- 数据预处理：

  - 去均值：$x = x-mean(x, axis=0)$
  - 归一化：x=x/std(x+1e-6, axis=0)​

- BatchNorm：对于每一层数据均进行归一化处理（先去均值再归一化）

  - 去均值：$x = x-mean(x, axis=0)$
  - 归一化：x=x/std(x+1e-6, axis=0)​
  - 两个可训练参数： x = alpha * x + beta （alpha: 认为归一化之后仍能学习到一部分数据相对大小的特征，beta：认为能学习到一部分均值的特征）

- 现在很多网络中都使用Batchnormal来加快训练速度，也能避免过拟合和梯度消失

- Relu+Batchnormal 在大部分网络中都是有效的

- batchnormal中的均值是来自所有样本，更新方式：

  - $\bar x_t= \beta \bar x_{t-1} + (1-\beta) mean(x)$          $\beta 通常取0.99$



自从BatchNormal论文问世之后sigmoid激活函数也被广泛使用



- 如何选择激活函数：
  - 无法根据理论去选择，参考论文中使用的激活函数，或者自己尝试



## 样本均衡问题（贷款欺诈预测）

- 需要样本均衡
  - 采样  
  - 加权



## 深度学习去缺点

- 容易过拟合：对数据量要求较高
- 容易出现梯度消失
- 计算速度慢



## 优化算法

- Adam
- RMSprop