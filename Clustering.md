### 相似度度量

1. 欧式距离
2. Jaccard相似系数   $J=\frac{|A \bigcap B|}{|A\bigcup B|}$
3. 余弦相似度 $cos(\theta)=\frac{a^Tb}{|a|\cdot|b|}$
4. 皮尔逊相关系数
5. 相对熵（K-L距离）
6. Hellinger距离





## K-means

假定输入样本为$S=x_1,x_2,\cdots ,x_m$

算法流程：

1. 选择初始的k个类别中心$\mu_1, \mu_2, \cdots,\mu_k$

2. 对每个样本$x_i$，将其标记为距离类别中心最近的类别（EM算法中M步），即：
   $$
   label_j=\arg \min_{1 \le j \le k}||x_i-\mu_j||
   $$

3. 将每个类别中心更新为隶属于该类别的所有样本的均值（EM算法E步）
   $$
   \mu_j = \frac{1}{|c_j|}\sum_{i\in c_j}x_i
   $$

4. 重复最后两步，知道类别中心的变化小于某阈值

终止条件：

​	迭代次数/簇中心变化率/最小平方误差MSE

### Kmeans算法总结

- 有点
  - 经典聚类算法，简单快速
  - 对处理大数据集，该算法保持可伸缩性和高效率
  - 当簇近似为高斯分布时，他的效果较好
- 缺点
  - 无法完成复杂形状的聚类
  - 必须是先给出K，而且对初值敏感
  - 对噪声和离群点敏感

1. 无法完成复杂形状的聚类



## GMM

假设数据服从高斯分布，	

相当于是对Kmeans的拓展，在中心的基础上，多了方差，比Kmeans的模型更复杂。是Kmeans算法的改进形式。



## 层次聚类



度量两个簇之间的距离：

1. 簇中心距离（一般选择）
2. 两个簇中最近的两个样本点的距离（容易受到噪声的影响，容易把两个簇合并）



## 多元高斯分布





## 聚类做图像分割

层次聚类的思路：两个相邻的像素点的距离比较近就分为一类，否则不是同一类



## 样本策略

- 弃真错误，取伪错误
- 例子：检测异常，大部分样本是正常的
- 直接输入到网络中训练
- 样本策略1：正常样本降采样
  
- 预测可能出现正样本被识别异常（正样本降采样容易导致一些特征没有采集到，导致网络不能学习到所所有正样本的所有特征。）
  
- 样本策略2：将正常样本的loss加权

  - 可以降低取伪（假阳性）错误概率

  通常策略1和策略2一块儿使用。

贝叶斯算法收样本不均衡的影响较小，可以不乘先验概率