贝叶斯算法是基于贝叶斯定理和特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入X，利用贝叶斯定理求出后验概率最大的输出y，不俗贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。



给定某系统的若干样本X，计算该系统的参数，即：
$$
P(\theta|x)=\frac{P(x|\theta)P(\theta)}{P(x)}
$$
$P(\theta)$：没有数据支持下，$\theta$发生的概率：先验概率

$P(\theta|x)$：在数据x的支持下，$\theta$发生的概率：后验概率

$P(x|\theta)$：给定某参数$\theta$的概率分布：似然函数



## 联合概率

$P(x_1,x_2)=P(x_1)P(x_2|x_1)=P(x_2)P(x_1|x_2)$

$P(x_1|x_2)=\frac{P(x_1)P(x_2|x_1)}{P(x_2)}$

其中$P(x_1)$叫做先验概率，$P(x_1|x_2)$叫做后验概率，$P(x_1,x_2)$叫做联合概率

分类问题：$p(类别|数据具有的属性)$

几何角度：$类别=f(数据具有的特征)$

## 朴素贝叶斯

假设：1、特征之间相互独立

对于给定的特征向量 $x_1,x_2,\ldots,x_n $

l类别$y$的概率可以根据贝叶斯公式得到：
$$
P(y|x_1,x_2,\ldots,x_n)=\frac{P(y)P(x_2,x_2,\ldots,x_n|y)}{P(x_1,x_2,\ldots,x_n)}
$$

- 使用朴素的独立性假设（特征之间相互独立）：

  $P(x_i|y,x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_m)=P(x_i|y)$

- 类别$y$的概率可简化为：

  $P(y|x_1,x_2,\ldots,x_n)=\frac{P(y)P(x_1,x_2,\dots,x_n)|y}{P(x_1,x_2,\ldots,x_n)}=\frac{P(y)\prod_{i=1}^nP(x_i|y)}{P(x_1,x_2,\ldots,x_n)}$

- 在给定样本的前提下，$P(x_1,x_2,\ldots,x_n)$是常数

  $P(y|x_1,x_2,\ldots,x_n)  与P(y)\prod_{i=1}^nP(x_i|y)成正比$

- 从而：
  $$
  \hat y=arg\,\max_{y} P(y)\prod_{i=1}^nP(x_i|y)​
  $$
  

#### 假设特征服从高斯分布（连续型特征）:

$$
P(x_i|y)=\frac{1}{\sqrt {2\pi}\sigma_y}\exp \left( \frac{(x_i-\mu_y)^2}{2\sigma_y^2} \right)
$$



每个特征都是一个单独的分布，上述公式中的$\sigma_y \,\mu_y$表示每个类别下每个特征的方差和均值

```python
import numpy as np
from sklearn import datasets
from sklearn.datasets import make_circles, make_classification
#获取数据
import matplotlib.pyplot as plt 
X, y = make_circles(noise=0.2, factor=0.5, random_state=1)
#X, y = make_moons(noise=0.3, random_state=0)
# X[N, 2]->Float
# y[N] -> int 
# 所用第0类的样本
X_1 = X[y==0]
# 所有的第1类的样本
X_2 = X[y==1]
# 对样本数量进行统计
n_x1 = len(X_1)
n_x2 = len(X_2)
# 计算先验概率
pc1 = (n_x1)/(n_x1+n_x2)
pc2 = (n_x2)/(n_x1+n_x2)

mupc1x1 = np.mean(X_1[:, 0])
mupc1x2 = np.mean(X_1[:, 1])
mupc2x1 = np.mean(X_2[:, 0])
mupc2x2 = np.mean(X_2[:, 1])
sigmapc1x1 = np.std(X_1[:, 0])
sigmapc1x2 = np.std(X_1[:, 1])
sigmapc2x1 = np.std(X_2[:, 0])
sigmapc2x2 = np.std(X_2[:, 1])
x_p = [0, 0] # 待预测样本
def N(x, mu, sigma):
	''' 高斯分布 '''
    return 1/np.sqrt(2*np.pi)/sigma*np.exp(-(x-mu)**2/2/sigma**2)
p1 = pc1*N(x_p[0], mupc1x1, sigmapc1x1)*N(x_p[1], mupc1x2, sigmapc1x2)
p2 = pc2*N(x_p[0], mupc2x1, sigmapc2x1)*N(x_p[1], mupc2x2, sigmapc2x2)
pf1 = p1/(p1+p2)
pf2 = p2/(p1+p2)
#绘制散点图
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
plt.axis("equal")
plt.show()
```



#### 假设特征服从多项式特征（离散型特征）：

$$
P(x_i|y)=\frac{N_{yi}+\alpha}{N_y+\alpha \cdot n}
$$

$N_{yi}$：第$y$类样本下第$i$个特征的个数

$N_y$：第$y$类样本的个数

$\alpha$：超参数 $\alpha=1$：Laplace平滑  $\alpha<1$：Lidstone平滑

$n$：特征的数量



## 例

样本：1000封邮件，没个邮件被标记为垃圾邮件或者非垃圾邮件

分类目标：给定第10001封邮件，确定他是垃圾邮件还是非垃圾邮件

类别c：垃圾邮件$c_1$，非垃圾邮件$c_2$

建立词汇表：

- 使用现有的单词词典
- 将所有邮件中出现的单词都统计出来，得到词典。
- 记单词数目为$N$

将每个邮件m向量化为长度为$N$的向量$X$

贝叶斯公式：$P(c|X)=P(c)P(X|c) / P(X)$

- $P(c_1)|x$



## 总结

贝叶斯算法的假设条件很多，只能作为简单的分类算法

贝叶斯学派，利用先验知识纠正样本偏差