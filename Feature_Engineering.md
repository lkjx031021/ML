## 样本不均衡的常用处理办法

- 假定样本数目A类比B类多，且严重失衡：
  - A类欠采样Undersampling
    - 随机欠采样
    - A类分成若干子集，分别与B类进行训练
    - 基于聚类的A类分割
  - B类过采样Oversampling
    - 避免欠采样造成的信息丢失
  - B类数据合成 Synthetic Data Generation
    - 随机插值得到新样本$(\frac{x_1^{(i)}+x_2^{(i)}}{2},\frac{x_1^{(j)}+x_2^{(j)}}{2})$
    - SMOTE（Synthetic Minority Over-sampling Technique）
  - 代价敏感学习 Cost Sensitive Learning
    - 降低A类权值，提高B类权值

## 利用随机森林建立计算样本相似度

原理：若两样本同时出现在相同的叶节点的次数越多，则二者越相似。

算法过程：

​		记样本个数为$N$，初始化$N × N$的零矩阵$S$，$S[i,j]$表示样本$i$和样本$j$的相似度。

​		对于m颗决策树形成的随机森林，遍历所有决策树的所有叶子节点：

​				记该叶子节点包含的样本为$sample[1,2,3,...,k]$，则$S[i][j]$累加1。

​					样本$i,j\in sample[1,2,\cdots,k]$

​					样本$i, j$出现在相同叶子节点的次数增加1次。

​		遍历结束，则S为样本间的相似度矩阵，形如
$$
\begin{array}{c|c}
 & \text{1} & \text{2} & \text{3} \\
\hline
\text{1} & 3 & 2 & 0 \\
\text{2} & 2 & 3 & 1 \\
\text{3} & 0 & 1 & 3 
\end{array}
$$


其中，每一行可以当作一个样本的特征向量，可以利用余弦相似度，欧氏距离，皮尔逊系数等多种方法计算样本相似度。

## 利用随机森林计算特征重要程度

出现次数越多特征越重要



## PCA

- 原理：通过线性变换使得任意两个列属性间线性相关性较弱
- 线性模型前台下无法完成非线性降维任务
- 问题：约束强

1. 简介

   主成分分析（PCA）是一种常用的无监督学习方法，这一方法利用正交变换把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量成为主成分。最早由Pearson于1901年提出，但是针对非随机变量，1933年由Hotelling推广到随机变量。

2. 计算步骤
   1. 计算协方差矩阵
   2. 求解协方差矩阵特征值和特征向量
   3. 去除比较小的特征值对应的特征向量



线性降维方法保留的信息比较多

线性模型降维的效果不会超过稀疏自编码器的效果



## ICA

- 原理：通过线性变化，两类属性间的分布尽可能不同



## DL（字典学习）

- 原理：通过线性变换后，属性是稀疏的，大部分是0 小部分是1
- 约束：输出是稀疏的



## LDA（隐狄利克雷分配模型）

- 原理：给矩阵分解起了个名字
- 文本分词之后向量化 形成矩阵M = T @ W
- 主题（Topic）：一篇文章可能由多个主题组成
  - 矩阵T：主题概率矩阵
- 主题-词（Topic-word）：一个主题下某个词出现的概率
  - W：主题-词
  - 如何生成一篇文章（词无顺序）
    - 第一步：先根据主题的概率选定一个主题t
    - 第二部：根据主题-词的概率选定一个字w
    - 这样生成了文章的第一个词、
    - 重复这个过程直到最后一个词，生成了一篇文章
    - 重复这个过程直到最后一篇文章，生成了整个语料

M = T · W   M:词频   T： 主题频率   W：主题-词频率

- 如何利用LDA做文本分类
  - 第一步：数据读取
  - 第二部：文本分词
  - 第三步：文本向量化 -> 稀疏矩阵 -> [文章个数, 词个数]
  - 第四步：LDA降维 -> 密集矩阵 -> [文章个数, 主题个数]
    - 可以直接做分类
  - 第五步：分类
- 实现： 变分推断 -> EM -> 含有隐变量的极大似然估计