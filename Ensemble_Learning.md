## XGBoost

### 总结

XGBoost相对于传统的GBDT，XGBoost使用了二阶信息，可以更快的在训练集上收敛。

XGBoost的实现中使用了并行计算，因此训练速度更快，同时它的原生语言为c/c++，



## Adaboost

Adaboost可以看作是采用指数损失函数的提升方法，其每个基函数的学习算法为前向分步算法；

AdaBoost的训练误差是一直输速度下滑的；

AdaBoost算法不需要事先知道下届$\gamma$，具有自适应性（Adaptive），他能自适应弱分类器的训练误差率







## 总结

Bagging能够减少训练方差（Variance），对于不剪枝的决策树、神经网络等学习器有良好的集成效果；

Boosting能够减少偏差（Bias），能够基于泛化能力较弱的学习器构造强学习器。

除了GBDT中使用关于分类器的一阶导数进行学习之外，也可以借鉴（拟）牛顿法的思路，使用二阶导数学习弱分类器。



# 集成学习

- 容易过拟合的分类器：高方差低偏差的分类器 
- 容易欠拟合的分类器：低方差高偏差的分类器 
- 优化思路1：降低过拟合分类器的方差-
  - Bagging 类
  - 将一系列容易过拟合的分类器取平均 
  - 例子：随机森林
  - 例子：DropOut 
- 优化思路2：降低欠拟合分类器的偏差 
  - Boosting 类 
  - 将一系列容易欠拟合的分类器取加权平均 
  - 例子：广度神经网络 
  - 例子：Adaboost